{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "TINY_TRAIN_FILE = \"../data/complaints_1k.csv\"\n",
    "TINY_TEST_FILE = \"../data/complaints_500.csv\"\n",
    "\n",
    "SMALL_TRAIN_FILE = \"../data/complaints_10k.csv\"\n",
    "SMALL_TEST_FILE = \"../data/complaints_3k.csv\"\n",
    "SMALL_VALIDATION_FILE = \"../data/complaints_1k.csv\"\n",
    "\n",
    "FULL_TRAIN_FILE = \"../data/complaints_no_NA.csv\"\n",
    "\n",
    "DEVELOPING = True\n",
    "# DEVELOPING = False\n",
    "\n",
    "if DEVELOPING:\n",
    "    BATCH_SIZE = 10\n",
    "    MAX_VOCAB_SIZE = 5000\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x): \n",
    "    '''\n",
    "    Tokenizer for torchtext. Right now, just splits on spaces\n",
    "    \n",
    "    Takes: string\n",
    "    Returns: list of strings\n",
    "    '''\n",
    "    \n",
    "    return x.split(\" \")\n",
    "\n",
    "\n",
    "def one_hot_encode_label(x):\n",
    "    '''\n",
    "    Converts string label into one hot encoded label\n",
    "\n",
    "    Takes: string\n",
    "    Returns: list with 1 in position corresponding to label \n",
    "    '''\n",
    "\n",
    "    if x == \"Closed with explanation\":\n",
    "        return [1, 0, 0, 0, 0, 0]\n",
    "    elif x == \"Closed with non-monetary relief\":\n",
    "        return [0, 1, 0, 0, 0, 0]\n",
    "    elif x == \"Closed with monetary relief\":\n",
    "        return [0, 0, 1, 0, 0, 0]\n",
    "    elif x == \"Untimely response\":\n",
    "        return [0, 0, 0, 1, 0, 0]\n",
    "    elif x == \"Closed\":\n",
    "        return [0, 0, 0, 0, 1, 0]\n",
    "    elif x == \"In progress\":\n",
    "        return [0, 0, 0, 0, 0, 1]\n",
    "    else:\n",
    "        print(\"Unexpected class label in one-hot encoding\")\n",
    "        print(x)\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def load_and_tokenize_data(path=TINY_TRAIN_FILE):\n",
    "    '''\n",
    "    turn csv of complaints -> pytorch data object\n",
    "\n",
    "    Takes: file path to data csv\n",
    "    Returns: \n",
    "    '''\n",
    "\n",
    "    # define which fields we want\n",
    "    data_fields = [('date_received', None),\n",
    "                   ('product', None),\n",
    "                   ('sub-product', None),\n",
    "                   ('issue', None),\n",
    "                   ('sub-issue', None),\n",
    "                   ('narrative', TEXT), # note this is the field name, not colname in csv\n",
    "                   ('company_public_response', None),\n",
    "                   ('company', None),\n",
    "                   ('state', None),\n",
    "                   ('zip_code', None),\n",
    "                   ('tags', None),\n",
    "                   ('consumer_consent_provided', None),\n",
    "                   ('submitted_via', None),\n",
    "                   ('date_sent_to_company', None),\n",
    "                   ('label', LABEL), # ditto here\n",
    "                   ('timely_response', None),\n",
    "                   ('consumer_disputed', None),\n",
    "                   ('complaint_id', None)]\n",
    "\n",
    "    return data.TabularDataset(path=path,\n",
    "                               format='csv',\n",
    "                               skip_header=True,\n",
    "                               fields=data_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing pipeline object\n",
    "OneHotEncoder = data.Pipeline(convert_token=one_hot_encode_label)\n",
    "\n",
    "# define text and label field objects with preprocessing\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=False, preprocessing=OneHotEncoder)\n",
    "\n",
    "train_data = load_and_tokenize_data(FULL_TRAIN_FILE)\n",
    "valid_data = load_and_tokenize_data(SMALL_VALIDATION_FILE)\n",
    "test_data = load_and_tokenize_data(SMALL_TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 5002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "# create embeddings\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key = lambda x: len(x.narrative),\n",
    "    sort_within_batch=False,\n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars(valid_iterator.dataset.examples[0])\n",
    "# it = iter(valid_iterator)\n",
    "# b0 = next(it)\n",
    "# dir(b0)\n",
    "# print(b0.narrative)\n",
    "# print(b0.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    '''\n",
    "    Define RNN model class\n",
    "    Source: Homework 3\n",
    "    \n",
    "    Parameters needed to initialize a new instance:\n",
    "    - type of RNN to train: text string, either LSTM or GRU\n",
    "    - number of tokens\n",
    "    - number of input dimensions\n",
    "    - hidden dimension\n",
    "    - number of layers desired\n",
    "    - dropout\n",
    "    - boolean to tie weights\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, rnn_type, vocab_size, embed_size, hidden_size, n_layers, n_tag, dropout=0.5):\n",
    "        ''' Initialize the following layers:\n",
    "            - Embedding layer/encoder\n",
    "            - Recurrent neural network layer (LSTM, GRU)\n",
    "            - Linear decoding layer to map from hidden vector to the vocabulary\n",
    "            - Optionally, dropout layers.  Dropout layers can be placed after \n",
    "              the embedding layer or/and after the RNN layer. \n",
    "            - Optionally, initialize the model parameters. \n",
    "            \n",
    "            Initialize a loss function\n",
    "            \n",
    "            Create attributes where model will store training time, loss info\n",
    "            \n",
    "        '''\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(embed_size, hidden_size, n_layers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, n_tag)\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_tag = n_tag\n",
    "        \n",
    "        # loss function\n",
    "        self.loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "        \n",
    "        \n",
    "        self.training_time = None\n",
    "        self.validation_loss = None\n",
    "\n",
    "    def initHidden(self):\n",
    "        '''\n",
    "        Initialize a hidden vector (None/zeroes) \n",
    "        '''\n",
    "        return None\n",
    "        \n",
    "    def forward(self, input, hidden0):\n",
    "        ''' \n",
    "        Run forward propagation for a given minibatch of inputs:\n",
    "        process through the embedding, RNN, and the decoding layer.\n",
    "        \n",
    "        Takes: input text, hidden vector (tuple)\n",
    "        Returns: decoded probability scores, hidden vector (tuple)\n",
    "\n",
    "        '''\n",
    "        \n",
    "        embeds = self.encoder(input)\n",
    "        output, hiddenn = self.rnn(embeds, hidden0) \n",
    "        decoded = self.decoder(output)\n",
    "\n",
    "        return decoded, hiddenn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    '''\n",
    "    Evaluate a model on data.\n",
    "    \n",
    "    Takes: model object, data iterator object\n",
    "    Returns: average cross entropy loss across all batches in data\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    it = iter(data)\n",
    "    total_count = 0. \n",
    "    total_loss = 0. \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Initialize hidden vector\n",
    "        hidden = model.initHidden() \n",
    "        \n",
    "        for i, batch in enumerate(it):\n",
    "                                    \n",
    "            # extract text and target for batch\n",
    "            batch_text = batch.narrative\n",
    "            target = batch.label\n",
    "\n",
    "            # if using a CUDA\n",
    "            if USE_CUDA:\n",
    "              batch_text = batch_text.cuda()\n",
    "        \n",
    "            # zero out gradients for current batch\n",
    "            model.zero_grad()\n",
    "\n",
    "            # call forward propagation\n",
    "            decoded, hiddenn = model(batch_text, hidden)\n",
    "\n",
    "            if model.rnn_type == \"LSTM\":\n",
    "                hidden = hiddenn[0], hiddenn[1]\n",
    "            else:\n",
    "                hidden = hiddenn\n",
    "\n",
    "            # reshape target so there is one one-hot-encoded vector per word?\n",
    "            words_in_batch, batch_size, C = decoded.shape\n",
    "            N = words_in_batch * batch_size\n",
    "            \n",
    "            # get average cross entropy loss for batch  = THIS MIGHT BE WRONG\n",
    "            loss = model.loss_fn(decoded, target)\n",
    "            total_loss += loss * N\n",
    "            \n",
    "            # count number of target words in batch (same dims as target)\n",
    "            total_count += N\n",
    "            \n",
    "                \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define desired dimensions for this specific run\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "NUM_EPOCHS = 1\n",
    "GRAD_CLIP = 1\n",
    "\n",
    "parameters = {\n",
    "    \"model_type\": \"LSTM\", \\\n",
    "    \"vocab_size\": INPUT_DIM, \\\n",
    "    \"embedding_size\": 40, \\\n",
    "    \"hidden_size\": 50, \\\n",
    "    \"num_layers\": 2, \\\n",
    "    \"n_categories\": 6, \\\n",
    "    \"dropout\": 0.5\n",
    "#     \"tie_weights\": False\n",
    "    }\n",
    "\n",
    "\n",
    "# # set loss and optmizer\n",
    "# loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with parameters:\n",
      "{'model_type': 'LSTM', 'vocab_size': 5002, 'embedding_size': 40, 'hidden_size': 50, 'num_layers': 2, 'n_categories': 6, 'dropout': 0.5}\n",
      "begin training at 1591467002.3987432\n",
      "\t \t loss at 100th tensor(0.6993, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2327, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0683, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2821, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1263, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2437, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2818, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1920, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1242, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2120, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1816, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1838, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3760, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1798, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1258, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1799, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2011, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1864, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2662, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2571, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2851, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1355, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1454, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2827, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1341, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1341, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0849, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2342, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3102, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1587, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2446, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2039, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3718, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1261, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1758, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3955, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1860, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1283, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3257, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1768, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2071, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1845, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3000, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1442, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1286, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1351, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1769, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2358, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2867, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1780, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1896, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1939, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2776, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2242, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1237, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2511, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2062, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2965, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2050, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1631, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2610, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2652, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2333, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0648, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2332, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0718, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2226, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1174, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1885, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1088, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2373, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1617, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1819, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1866, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1855, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1204, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1681, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2562, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1741, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2423, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1782, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1344, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1510, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1264, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2762, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2252, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3329, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2839, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1314, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1170, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2983, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1833, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3700, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0927, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1863, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2489, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3613, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2980, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2176, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2729, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2125, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1286, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2109, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0796, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2096, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1810, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1751, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2856, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1953, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2256, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1779, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2228, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2795, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1171, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3188, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2375, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0727, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \t loss at 100th tensor(0.2567, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2243, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2271, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2081, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1186, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2188, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2229, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2239, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2446, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2865, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1729, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1840, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1118, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1633, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2394, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0702, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1578, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1850, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2355, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1910, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2238, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2926, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1767, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1218, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2300, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1733, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2554, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1739, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2940, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2305, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1196, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1611, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1832, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1918, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1243, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2981, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1755, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1648, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2363, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1748, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1824, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2813, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2372, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1476, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1963, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1208, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0654, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2784, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1683, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1918, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2381, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1140, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1113, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1209, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1180, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3343, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1158, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2535, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1215, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1443, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1260, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2717, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1327, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2875, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1138, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2208, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1631, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2482, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2305, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1174, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1728, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2167, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1571, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2540, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1670, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1751, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1646, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1719, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1853, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1090, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1067, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1774, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3713, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2173, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2447, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3485, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1534, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1967, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1278, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1231, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2012, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1906, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1534, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1387, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2346, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2389, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1716, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1675, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1622, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2342, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1364, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1236, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2288, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3321, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3027, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2720, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1280, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2440, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3323, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1499, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2679, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2958, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3581, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2442, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1314, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1253, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2252, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2355, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1459, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2634, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \t loss at 100th tensor(0.1141, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1922, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1619, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1294, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2499, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1687, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1198, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2896, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1961, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1810, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2648, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0510, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1731, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1658, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2432, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1325, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1773, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1244, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2168, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2595, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1281, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2487, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2409, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2450, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1153, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1690, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1177, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1746, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0567, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1290, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2044, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1557, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1772, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1901, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1425, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2295, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1420, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1299, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2550, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1141, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1368, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0499, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1184, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1180, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2045, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2240, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2335, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1954, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3944, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3848, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2437, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1858, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1672, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1703, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0513, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1133, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1841, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0655, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2219, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1692, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2295, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1850, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1799, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3234, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2448, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1151, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1878, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2090, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2637, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1313, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1943, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0616, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1265, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3048, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1942, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1303, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2091, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3038, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1956, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1431, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1312, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1101, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3140, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3632, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2810, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1604, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1839, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2054, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1960, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1218, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1251, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2735, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2110, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1082, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2373, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1731, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1215, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1718, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1711, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1335, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1305, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2837, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2086, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1761, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1947, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1805, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2287, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1261, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1661, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1552, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1127, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2281, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1069, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2149, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1689, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1511, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1170, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2564, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1510, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \t loss at 100th tensor(0.3241, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1395, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1809, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3026, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1980, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2194, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3926, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3498, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2349, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2056, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1424, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1768, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2852, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1234, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1882, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2706, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1655, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1971, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1578, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.5069, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1171, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1744, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2246, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2085, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2430, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2539, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1944, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1470, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1852, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3012, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2124, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1333, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1654, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1990, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1800, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3234, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2127, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2461, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1367, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1678, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2789, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2059, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2119, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2754, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2499, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2126, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2298, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2683, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1890, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.3009, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1203, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1277, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1664, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1700, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1523, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1422, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1109, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1389, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1635, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2388, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1164, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1876, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2534, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1766, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.4466, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2381, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1964, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1305, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1735, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1490, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1091, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2520, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1083, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2605, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1820, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2253, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2284, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2449, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2260, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.1856, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.2834, grad_fn=<MeanBackward0>)\n",
      "\t \t loss at 100th tensor(0.0613, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 4, 50), got (2, 10, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-26228a4a21ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mbest_model_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-26228a4a21ee>\u001b[0m in \u001b[0;36mget_best_model\u001b[0;34m(parameters, train_iter, val_iter)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# call forward propagation and detach both hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddenn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-2104f7a99468>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden0)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddenn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 523\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    524\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    525\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 4, 50), got (2, 10, 50)"
     ]
    }
   ],
   "source": [
    "def get_best_model(parameters, train_iter=train_iterator, val_iter=valid_iterator):\n",
    "    '''\n",
    "    Find the model with the lowest validation error, given parameters\n",
    "\n",
    "    Takes: parameter dict\n",
    "    Returns: best model state dict, average cross entropy loss on validation\n",
    "    '''\n",
    "\n",
    "    print(\"Training model with parameters:\")\n",
    "    print(parameters)\n",
    "\n",
    "    model = RNNModel(*list(parameters.values()))\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    val_losses = []\n",
    "    best_model_dict = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"begin training at\", start_time)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        it = iter(train_iter)\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "        hidden = None\n",
    "        for i, batch in enumerate(it):\n",
    "            ''' Do the following:\n",
    "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
    "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "                  the current batch. But detach each tensor in the hidden state vector using tensor.detach(). See\n",
    "                  https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach. \n",
    "                - Zero out the model gradients to reset backpropagation for current batch\n",
    "                - Call forward propagation to get output and final hidden state vector.\n",
    "                - Compute the cross entropy loss\n",
    "                - Run back propagation to set the gradients for each model parameter.\n",
    "                - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
    "                  https://pytorch.org/docs/stable/nn.html#clip-grad-norm\n",
    "                - Run a step of gradient descent. \n",
    "                - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
    "                - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
    "                  your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
    "                  copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
    "                  https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
    "                  in Sec 2.3.1 of Lecture notes by Cho: \n",
    "                  https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
    "            '''\n",
    "            \n",
    "#             print(\"\\t Beginning batch\", i)\n",
    "            \n",
    "            # extract narrative and label for batch\n",
    "            batch_text = batch.narrative\n",
    "            target = batch.label\n",
    "\n",
    "            # if using a CUDA\n",
    "            if USE_CUDA:\n",
    "              batch_text = batch_text.cuda()\n",
    "            \n",
    "            # zero out gradients for current batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # call forward propagation and detach both hidden layers\n",
    "            decoded, hiddenn = model(batch_text, hidden)\n",
    "\n",
    "            if model.rnn_type == \"LSTM\":\n",
    "                hidden = hiddenn[0].detach(), hiddenn[1].detach()\n",
    "            else:\n",
    "                hidden = hiddenn.detach()\n",
    "            \n",
    "            # reshape target so there is one one-hot-encoded vector per word?\n",
    "            words_in_batch, batch_size, C = decoded.shape\n",
    "            N = words_in_batch * batch_size\n",
    "\n",
    "#             print(\"\\t \\t decoded shape\", decoded.shape)\n",
    "#             print(\"\\t \\t target shape\", target.shape)\n",
    "\n",
    "\n",
    "            # compute cross entropy loss \n",
    "            loss = model.loss_fn(decoded, target)\n",
    "\n",
    "\n",
    "            # print batch loss every 1000 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\t \\t loss at 100th\", loss)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm = GRAD_CLIP) \n",
    "            optimizer.step()\n",
    "            \n",
    "            # evaluate model every 1000 iterations\n",
    "            if i % 1000 == 0:\n",
    "                \n",
    "                # compute loss, see if this is best model, append loss to loss list\n",
    "                current_loss = evaluate(model, val_iter)\n",
    "                \n",
    "                if len(val_losses) == 0 or current_loss < min(val_losses):\n",
    "                    best_model_dict = model.state_dict()\n",
    "                \n",
    "                val_losses.append(current_loss)\n",
    "            \n",
    "    print(\"Training complete.\")\n",
    "    print(\"Training time: \", time.time() - start_time)\n",
    "\n",
    "    model.train_time = time.time() - start_time\n",
    "\n",
    "    return best_model_dict, time.time() - start_time\n",
    "\n",
    "best_model_dict, train_time = get_best_model(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "def save_model(model_dict, filename):\n",
    "    '''\n",
    "    Save best model state dictionary in Google drive\n",
    "\n",
    "    Takes:\n",
    "    - model state_dict() object\n",
    "    - string filename\n",
    "    Returns: None\n",
    "    '''\n",
    "    path = F\"../models/{filename}\" \n",
    "    torch.save(model_dict, path)\n",
    "\n",
    "\n",
    "def load_model(params, filename):\n",
    "    '''\n",
    "    Load trained model from saved state dictionary in Google Drive\n",
    "\n",
    "    Takes:\n",
    "    - dictionary of parameters\n",
    "    - string filename (no path needed)\n",
    "    Returns:\n",
    "    - model object\n",
    "    '''\n",
    "    best_model = RNNModel(*list(parameters.values()))\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        best_model = best_model.cuda()\n",
    "    \n",
    "    path = F\"../models/{filename}\" \n",
    "    best_model.load_state_dict(torch.load(path), strict=False)\n",
    "    return best_model\n",
    "\n",
    "save_model(best_model_dict, \"dev_rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
    "'''\n",
    "\n",
    "# load best model by creating new instance of model type and loading state_dict\n",
    "best_model = load_model(parameters, \"dev_rnn.pt\")\n",
    "\n",
    "best_model_val_perplexity = torch.exp(evaluate(best_model, valid_iterator))\n",
    "print(\"Perplexity of best model on validation set:\", best_model_val_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate the loss of best_model on the testing set and compute its perplexity.\n",
    "'''\n",
    "best_model_test_perplexity = torch.exp(evaluate(best_model, test_iterator))\n",
    "print(\"Perplexity of best model on testing set:\", best_model_test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
