{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 1.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchtext) (4.44.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchtext) (1.14.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchtext) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->torchtext) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->torchtext) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->torchtext) (2.9)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->torchtext) (0.18.2)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.91 torchtext-0.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "import util as util\n",
    "from models import *\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# TO DO: improve this structure\n",
    "DEVELOPING = True\n",
    "# DEVELOPING = False\n",
    "WHICH_TASK = \"product\"\n",
    "\n",
    "if DEVELOPING:\n",
    "    # in order: train, validation, test\n",
    "    files = [\"../data/complaints_3k.csv\", \\\n",
    "                \"../data/complaints_500.csv\", \\\n",
    "                \"../data/complaints_1k.csv\"]\n",
    "    BATCH_SIZE = 7\n",
    "    MAX_VOCAB_SIZE = 5000\n",
    "else:\n",
    "    # in order: train, validation, test\n",
    "    files = [\"../data/full_training_set.csv\", \\\n",
    "                \"../data/full_validation_set.csv\", \\\n",
    "                \"../data/full_testing_set.csv\"]    \n",
    "    BATCH_SIZE = 64\n",
    "    MAX_VOCAB_SIZE = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tokenize_data(path, TEXT, LABEL, which_task):\n",
    "    '''\n",
    "    turn csv of complaints -> pytorch data object\n",
    "\n",
    "    Takes: \n",
    "    - file path to data csv\n",
    "    - TEXT field defini sction object\n",
    "    - LABEL field definition object\n",
    "    - string denoting which field is label (\"response\" or \"product\")\n",
    "    Returns: \n",
    "    - Tabular dataset type\n",
    "    '''\n",
    "\n",
    "    # define fields\n",
    "    if which_task == \"response\":\n",
    "        data_fields = [('date_received', None),\n",
    "                    ('product', None),\n",
    "                    ('sub-product', None),\n",
    "                    ('issue', None),\n",
    "                    ('sub-issue', None),\n",
    "                    ('narrative', TEXT), # note this is the field name, not colname in csv\n",
    "                    ('company_public_response', None),\n",
    "                    ('company', None),\n",
    "                    ('state', None),\n",
    "                    ('zip_code', None),\n",
    "                    ('tags', None),\n",
    "                    ('consumer_consent_provided', None),\n",
    "                    ('submitted_via', None),\n",
    "                    ('date_sent_to_company', None),\n",
    "                    ('label', LABEL), # ditto here\n",
    "                    ('timely_response', None),\n",
    "                    ('consumer_disputed', None),\n",
    "                    ('complaint_id', None)]\n",
    "    else:\n",
    "        data_fields = [('date_received', None),\n",
    "                    ('label', LABEL), # ditto here\n",
    "                    ('sub-product', None),\n",
    "                    ('issue', None),\n",
    "                    ('sub-issue', None),\n",
    "                    ('narrative', TEXT), # note this is the field name, not colname in csv\n",
    "                    ('company_public_response', None),\n",
    "                    ('company', None),\n",
    "                    ('state', None),\n",
    "                    ('zip_code', None),\n",
    "                    ('tags', None),\n",
    "                    ('consumer_consent_provided', None),\n",
    "                    ('submitted_via', None),\n",
    "                    ('date_sent_to_company', None),\n",
    "                    ('company_response_to_consumer', None), \n",
    "                    ('timely_response', None),\n",
    "                    ('consumer_disputed', None),\n",
    "                    ('complaint_id', None)]\n",
    "\n",
    "    return data.TabularDataset(path=path,\n",
    "                               format='csv',\n",
    "                               skip_header=True,\n",
    "                               fields=data_fields)\n",
    "\n",
    "\n",
    "def preprocess(which_task, train_file, val_file, test_file, max_vocab_size=MAX_VOCAB_SIZE):\n",
    "    '''\n",
    "    Load data and preprocess:\n",
    "    - apply tokenization\n",
    "    - one hot encode labels\n",
    "    - build embeddings\n",
    "\n",
    "    Takes:\n",
    "    - string denoting which field is label (\"response\" or \"product\")\n",
    "    - filename of training data csv\n",
    "    - filename of validation csv\n",
    "    - filename of testing csv\n",
    "    - max vocab size\n",
    "    Returns:\n",
    "    - train data, validation data, test data object\n",
    "    '''\n",
    "\n",
    "    if which_task not in [\"response\", \"product\"]:\n",
    "        print(\"preprocessing error: which field is the label?\")\n",
    "        raise ValueError\n",
    "\n",
    "    # define text field objects with tokenization\n",
    "    TEXT = data.Field(sequential=True, tokenize=util.tokenize, lower=True)\n",
    "\n",
    "    # define label field with one hot encoded labels\n",
    "    if which_task == \"response\":\n",
    "        OneHotEncoder = data.Pipeline(convert_token=util.one_hot_encode_response)\n",
    "        LABEL = data.LabelField(sequential=False, use_vocab=False, preprocessing=OneHotEncoder)\n",
    "    else:\n",
    "        OneHotEncoder = data.Pipeline(convert_token=util.one_hot_encode_product)\n",
    "        LABEL = data.LabelField(sequential=False, use_vocab=False, preprocessing=OneHotEncoder)\n",
    "\n",
    "\n",
    "    train_data = load_and_tokenize_data(train_file, TEXT, LABEL, which_task)\n",
    "    valid_data = load_and_tokenize_data(val_file, TEXT, LABEL, which_task)\n",
    "    test_data = load_and_tokenize_data(test_file, TEXT, LABEL, which_task)\n",
    "\n",
    "    # create embeddings from training data\n",
    "    TEXT.build_vocab(train_data, max_size=max_vocab_size)\n",
    "    LABEL.build_vocab(train_data)\n",
    "    print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "    print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "def optimize_params(parameters, train_iter, val_iter):\n",
    "    '''\n",
    "    Find model parameters with the lowest validation error\n",
    "\n",
    "    - Extract narrative word list and label from the batch\n",
    "    - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "        the current batch, detaching\n",
    "    - Zero out the model gradients to reset backpropagation for current batch\n",
    "    - Call forward propagation to get output and final hidden state vector.\n",
    "    - Compute loss\n",
    "    - Run back propagation to set the gradients for each model parameter.\n",
    "    - Clip the gradients that may have exploded. \n",
    "    - Evaluate model on the validation set periodically\n",
    "\n",
    "    Takes: \n",
    "    - parameter dict\n",
    "    Returns: \n",
    "    - best model\n",
    "    - average loss on validation set\n",
    "    '''\n",
    "\n",
    "    print(\"Training model with parameters:\")\n",
    "    print(parameters)\n",
    "\n",
    "    model = RNNModel(*list(parameters.values()))\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # TO DO: maybe don't want to set these here\n",
    "    learning_rate = 0.01 # 0.001 \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    last_batch_size = BATCH_SIZE\n",
    "\n",
    "    val_losses = []\n",
    "    best_model = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"begin training at\", start_time)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        it = iter(train_iter)\n",
    "        hidden = model.initHidden()\n",
    "        for i, batch in enumerate(it):\n",
    "\n",
    "            # extract narrative and label for batch\n",
    "            batch_text = batch.narrative\n",
    "            target = batch.label\n",
    "\n",
    "            # drop last batch if it is too short\n",
    "            # happens when number of narratives not divisible by batch size\n",
    "            if i > 0 and batch_text.shape[1] != last_batch_size:\n",
    "                break\n",
    "\n",
    "            # if using a CUDA, put text on CUDA\n",
    "            if USE_CUDA:\n",
    "                batch_text = batch_text.cuda()\n",
    "            \n",
    "            # zero out gradients for current batch + call forward\n",
    "            model.zero_grad()\n",
    "            decoded, hiddenn = model(batch_text, hidden)\n",
    "\n",
    "            # detach  hidden layers\n",
    "            if model.rnn_type == \"LSTM\":\n",
    "                hidden = hiddenn[0].detach(), hiddenn[1].detach()\n",
    "            else:\n",
    "                hidden = hiddenn.detach()\n",
    "\n",
    "            # keep track of batch size\n",
    "            last_batch_size = batch.batch_size\n",
    "\n",
    "            # compute cross entropy loss \n",
    "            loss = model.loss_fn(decoded, target)\n",
    "\n",
    "            # print batch loss every 500 iterations\n",
    "            if i % 500 == 0:\n",
    "                print(F\"\\t \\t loss at {i}th\", loss)\n",
    "\n",
    "            # backpropagation + clip gradients + one step\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm = GRAD_CLIP) \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "            # evaluate model every 1000 iterations\n",
    "            if i % 10000 == 0:\n",
    "                \n",
    "                # compute loss, see if this is best model, append loss to loss list\n",
    "                current_loss = model.evaluate(val_iter, BATCH_SIZE)\n",
    "                \n",
    "                if len(val_losses) == 0 or current_loss < min(val_losses):\n",
    "                    best_model = model\n",
    "                \n",
    "                val_losses.append(current_loss)\n",
    "            \n",
    "    print(\"Training complete.\")\n",
    "    print(\"Training time: \", time.time() - start_time)\n",
    "    \n",
    "    return best_model, time.time() - start_time\n",
    "\n",
    "\n",
    "def run_model(which_task, model_params, train_iter, valid_iter, test_iter, save=False, model_file=''):\n",
    "    '''\n",
    "    Trains single model w/ given parameters and evaluates on test\n",
    "\n",
    "    Takes:\n",
    "    - string denoting which field is label (\"response\" or \"product\")\n",
    "    - dict of model parameters\n",
    "    - train iterable of batches\n",
    "    - validation iterable of batches\n",
    "    - test iterable of batches\n",
    "    - filename for model state dict\n",
    "    - boolean to turn off saving model state dict\n",
    "    '''\n",
    "\n",
    "    best_model, train_time = optimize_params(model_params, train_iter, valid_iter)\n",
    "    \n",
    "    # compute loss on test set\n",
    "    test_loss = best_model.evaluate(test_iter, BATCH_SIZE)\n",
    "    print(\"Loss of best model on testing set:\", test_loss)\n",
    "\n",
    "    # save state\n",
    "    if save:\n",
    "        optimized_dict = best_model.state_dict()\n",
    "        util.save_model(optimized_dict, model_file)\n",
    "\n",
    "    return best_model, train_time, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 5002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "Training model with parameters:\n",
      "{'model_type': 'LSTM', 'vocab_size': 5002, 'embedding_size': 40, 'hidden_size': 50, 'num_layers': 2, 'n_categories': 18, 'dropout': 0.5}\n",
      "begin training at 1591585596.421015\n",
      "\t \t loss at 0th tensor(0.7074, grad_fn=<MeanBackward0>)\n",
      "Training complete.\n",
      "Training time:  117.6197760105133\n",
      "Loss of best model on testing set: tensor(0.1818)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = preprocess(WHICH_TASK, *files)\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits( \\\n",
    "(train_data, valid_data, test_data), \\\n",
    "sort_key = lambda x: len(x.narrative), \\\n",
    "sort_within_batch=False, \\\n",
    "batch_size = BATCH_SIZE) \n",
    "\n",
    "iters = (train_iter, valid_iter, test_iter)\n",
    "\n",
    "'''\n",
    "DO MODEL RUNS\n",
    "'''\n",
    "\n",
    "# TO DO: do something better with these guys\n",
    "USE_CUDA = False\n",
    "INPUT_DIM = MAX_VOCAB_SIZE + 2 # this is janky\n",
    "NUM_EPOCHS = 1\n",
    "GRAD_CLIP = 1\n",
    "\n",
    "\n",
    "company_response_parameters = {\n",
    "    \"model_type\": \"LSTM\", \\\n",
    "    \"vocab_size\": INPUT_DIM, \\\n",
    "    \"embedding_size\": 40, \\\n",
    "    \"hidden_size\": 50, \\\n",
    "    \"num_layers\": 2, \\\n",
    "    \"n_categories\": 5, \\\n",
    "    \"dropout\": 0.5\n",
    "#     \"tie_weights\": False # didn't implement this but we could\n",
    "}\n",
    "\n",
    "product_parameters = {\n",
    "    \"model_type\": \"LSTM\", \\\n",
    "    \"vocab_size\": INPUT_DIM, \\\n",
    "    \"embedding_size\": 40, \\\n",
    "    \"hidden_size\": 50, \\\n",
    "    \"num_layers\": 2, \\\n",
    "    \"n_categories\": 18, \\\n",
    "    \"dropout\": 0.5\n",
    "#     \"tie_weights\": False # didn't implement this but we could\n",
    "}\n",
    "\n",
    "# this can get put into a loop, if it doesn't run insanely slowly\n",
    "# best_model, train_time, test_loss = run_model(WHICH_TASK, company_response_parameters, *iters, save=False)\n",
    "best_model, train_time, test_loss = run_model(WHICH_TASK, product_parameters, *iters, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
